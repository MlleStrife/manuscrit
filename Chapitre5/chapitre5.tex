\chapter{Approches retenues sur la modalité acoustique : la baseline}
\label{chapitre5}

\section{Motivation}
Dans ce chapitre de contribution, nous allons détailler les différentes procédures et tous les questionnements que nous avons eu lors de la réalisation de systèmes de reconnaissance d'émotions dans la parole.
Comme nous l'avons vu dans le chapitre~\ref{chapitre3}, le domaine du \textit{Speech Emotion Recognition} continue d'évoluer afin de proposer des systèmes de plus en plus performant.

Dans le cadre de cette thèse, nous nous inscrivons également dans un contexte industriel. L'entreprise Allo-Media, partenaire de cette CIFRE, a exprimé des besoins spécifiques quant aux émotions que nous devons traiter. Spécialisée dans le traitement d'informations issues de conversations téléphoniques entre un client et un conseiller, nous avons donc dû nous inscrire dans ce domaine d'analyse. Comme nous l'avons expliqué dans le chapitre~\ref{chapitre4}, nous avons choisi de nous focaliser sur deux émotions : la satisfaction et la frustration. Ces deux émotions, considérées comme deux émotions opposées dans le cadre de la relation clientèle, sont d'autant plus significatives si l'on analyse leurs variations au fil de la conversation.

Plusieurs facteurs ont donc guidés les choix que nous avons mis en place dans nos systèmes de reconnaissance. Tout d'abord, nous faisons face à du signal de basse qualité, puisque provenant d'une source téléphonique. Ensuite nous avions les problématiques de données personnelles dont nous avons parlé au chapitre~\ref{chapitre4}. Et enfin il faut considérer le temps de traitement qui ne doit pas être trop long pour pouvoir être diffusé rapidement après la fin de la conversation.

Cette problématique de reconnaissance de ces émotions précises dans un espace continu et ces contraintes nous ont conduit à mener plusieurs expérimentations, tout en considérant les solutions industrielles qui peuvent en découler.

\section{Nos positionnements}

\subsection{Annotation moyenne ou 3 annotations ?}
Comme nous l'avons vu dans le chapitre~\ref{chapitre4}, nous avons choisi de faire annoter le corpus par trois annotateurs. Ce choix a été motivé par l'aspect subjectif non négligeable de toutes les tâches mettant en œuvre des émotions. Nos données contiennent donc des conversations qui ont été annotées trois fois. Deux positionnements peuvent être pris à partir de ces annotations.
\begin{itemize}
  \item Nous pouvons considérer chaque annotation de chaque annotateur indépendamment. En effet, nous pouvons partir du principe que chaque annotateur détient sa propre version de l'émotion transmise dans les données et que cette version est légitime. Ce positionnement va donc considérer que l'individu a plus de valeur que le groupe entier. Nous pouvons donc spécifier nos systèmes de reconnaissance pour reproduire le comportement d'un humain $h$.
  \item Nous pouvons également considérer la moyenne des annotations. En effet, faire la moyenne des annotations revient à généraliser ces dernières pour qu'elles soient plus en adéquation avec un ensemble d'individus. Ainsi, nous allons considérer que l'annotation de référence pour nos systèmes automatiques est la moyenne de l'avis de différentes personnes. Bien que ce procédé soit plus pertinent lorsque nous avons un nombre élevé d'annotateurs, il reste cohérent pour trois annotateurs et permet de généraliser nos systèmes de reconnaissance pour reproduire non pas le comportement d'un humain $h$ mais d'un groupe d'humain $\sum_{i=1}^{n} h_i$.
\end{itemize}

Bien que nous ayons mené des expérimentations selon les deux prismes, nous avons décidé, en adéquation avec les besoins industriels, de nous concentrer sur le deuxième positionnement, comme nous l'avons énoncé dans le chapitre~\ref{chapitre4}. Ce positionnement est également celui de nombreux travaux dans le domaine, notamment les travaux effectués sur les corpus RECOLA et SEWA.

Nous nous sommes également posé la question de notre positionnement vis à vis de la double annotation : discrète et continue.

\subsection{Prédiction des émotions discrètes ou continues ?}
Le corpus étant annoté de façon discrète et continue, nous nous sommes posé la question de la pertinence de ces deux annotations dans le cadre industriel. En effet, un système de reconnaissance portant sur les étiquettes discrètes est plus facile à mettre en place et moins coûteux en ressources, comme nous avons moins d'étiquettes à prédire. Quelque soit la durée de la conversation, seules trois étiquettes sont à prédire : le niveau de frustration/satisfaction de début, de fin de conversation et son type d'évolution entre les deux.

Nous avons donc considéré dans un premier temps une prédiction d'étiquettes discrètes. En effet, cela nous permettait d'avoir une première approche sur la satisfaction et la frustration qui peut être utilisée dans un contexte industriel. Nous avons imaginé un système d'alerte permettant de mettre en avant les conversations dites \emph{préoccupantes}, que nous avons détailler dans la prochaine section.

Ces premiers travaux nous ont permis de préciser les besoins de l'entreprise. Ce système d'alerte ne permet pas de trouver assez rapidement les points \emph{préoccupants} d'une conversation. En effet, savoir situer dans une conversation les changements importants de satisfaction et surtout l'émergence de la frustration peut aider les entreprises à comprendre et adapter leur discours. Nous avons donc choisi de nous positionner dans un cadre continu pour la reconnaissance de l'axe de satisfaction, afin de permettre le développement d'outils mettant en exergue les points saillants des conversations.

Une fois notre positionnement trouvé, nous avions besoin de mettre en place une évaluation robuste afin de mesurer la performance de nos futurs systèmes.

\section{Construction d'une alerte sur les prédictions discrètes}
Notre premier travail préliminaire, afin de juger de la difficulté de la tâche et de la pertinence du corpus, a été de mettre en place une alerte sur les conversations dites \emph{préoccupantes}. Nous avons donc mis en place une classification supervisée de reconnaissance de la satisfaction et de la frustration discrète. Nous avons tout d'abord cherché à prédire l'étiquette de fin de conversation. Comme les étiquettes de début de conversation ne contiennent quasiment que des états neutres, il ne nous semblait pas constructif de construire un système de classification autour de ces données.

Pour mettre en place cette alerte, nous avons fait le choix de travailler avec plusieurs ensembles de descripteurs. En effet, comme nous l'avons déjà exprimé auparavant, les états émotionnels sont détectables à la fois dans l'acoustique et la linguistique. Ces descripteurs sont décrits dans le tableau~\ref{tab:descripteursDiscrets}.

\input{./Chapitre5/tables/descripteursDiscrets}

La modalité acoustique est représentée par le set d'INTERSPEECH 2009~\cite{OPENSMILE} et la modalité linguistique par un TF-IDF. Comme le nombre de descripteurs est très grand pour une classification de 303 documents, nous avons décidé de faire de la sélection de descripteurs afin d'éviter le sur-apprentissage notamment~\cite{Tahon2016}. Cette sélection est effectuée avec l'outil scikit-learn~\cite{scikitlearn} selon un seuil de variance (feature\_selection.VarianceThreshold) et un classement de \textit{mutual information} (feature\_selection.RFE).

Nous avons également fait le choix de considérer les deux modalités en même temps, en fusionnant les deux ensembles de descripteurs. Pour cette fusion nous avons testés différentes modalités de normalisation et nous avons choisi de conserver la normalisation standard puisqu'elle donne les meilleurs résultats.

En ce qui concerne les systèmes de Machine Learning mis en place, nous avons décidé d'utiliser la régression logistique et un SVM. Ces choix proviennent de précédentes expérimentations que nous avions mis en place sur une classification antérieure qui ne sera pas présentée dans ce manuscrit. Ces deux classifieurs sont implémenté en utilisant l'outil scikit-learn également. %Pour plus d'information sur leur configuration, le code de cette classification est disponible en annexe~\ref{an:codeFirstClassif}.

La classification se fait en validation croisée (\textit{k-folds}), c'est à dire que nous avons divisé les conversations du train et du dev en $k$ échantillons, ici cinq. L'ensemble de validation va changer, une fois l'apprentissage terminé. On va donc faire cinq apprentissages en quelque sorte. Le score final correspond à la moyenne des scores de ces cinq scores de performance. Ce procédé est très utilisé lorsque l'on craint un sur-apprentissage, notamment à cause du manque de données.

\input{./Chapitre5/tables/resultClassifDiscrete}

Les meilleurs performances de ces systèmes sont indiquées dans le tableau~\ref{tab:resultClassifDiscrete}. Les systèmes ont été évalués avec les mesures de précision et rappel non pondérées, que nous avons introduit au chapitre~\ref{chapitre3}. Nous pouvons observer que la régression logistique donne des meilleurs résultats sur la modalité linguistique et la fusion des deux modalités. Nous voyons également que la modalité linguistique donne toujours de meilleurs résultats. Nous observons également qu'en excluant les septs conversations satisfaites, nous améliorons fortement le score, passant d'un maximum de $0.52$ à $0.78$. En effet, il n'y a que sept conversations satisfaites dans nos jeux de données.

Bien que simple, cette première classification nous a permis de comprendre la difficulté de la tâche de reconnaissance des émotions. Elle a également permis de confirmer l'importance de la modalité linguistique, dont nous parlerons dans le chapitre~\ref{chapitre6}. Fort de ces expérimentations, nous avons mis en place une reconnaissance des émotions continues.

\section{Reconnaissance des émotions continues sur séquences de taille fixe}
Afin de s'aligner sur les architectures neuronales des articles de référence~\cite{Schmitt2019,SEWA} et des campagnes AVEC~\cite{AVEC2017,AVEC2018,AVEC2019}, nous avons choisi d'utiliser une taille de séquence d'entrée fixe. En effet, la sous partie de SEWA à laquelle nous avons accès ne comporte que des conversations de durée inférieures à trois minutes. De plus, un padding circulaire est utilisé pour ramener toutes ces conversations à une durée de trois minutes dans la plupart des travaux.

Comme nous l'avons vu dans le chapitre~\ref{chapitre4}, les conversations ont des durées variant de 32 secondes à 41 minutes avec une moyenne ($MOY$) de 7m24s et un écart type ($STD$) de 4m58s. Habituellement, la taille d'entrée est fixée à $MOY + STD$ (ici 12m22s) pour couvrir plus de 95\% du corpus. Les séquences longues sont alors coupées à la $MOY + STD$ tandis que les séquences courtes sont rallongées avec un padding. Dans notre cas, cela reviendrait à traiter des conversation de 12m22s.

Afin de réduire l'effet du padding et la durée d'apprentissage, nous avons décidé de fixer la durée de la séquence d'entrée à sept minutes. Nous avons appliqué un padding circulaire sur les courtes séquences.

\subsection{Exploration des ensembles eGeMAPS}
Pour mieux comparer notre travail avec l'état de l'art dans le domaine du SER, nous avons décidé d'utiliser l'ensemble eGeMAPS~\cite{Eyben2016}, que nous avons détaillé dans le chapitre~\ref{chapitre3}. Pour rappel, cet ensemble est constitué de descripteurs de bas niveau et de fonctions mathématiques appliquées sur ces derniers. Cet ensemble regroupe 88 descripteurs.

Nous avons également choisi d'utiliser un autre ensemble, dérivé de eGeMAPS. Dans les travaux de Schmitt et al.~\cite{Schmitt2019}, un sous ensemble, appelé f\_eGeMAPS, a été défini à partir de 23 LLD et des fonctions mathématiques appliquées sur ces LLD (principalement moyenne et STD). Il totalise 46 descripteurs. Un dernier descripteur, fonctionnant comme une détection de voix (\textit{voice activity detection} i.e. vad), dénotant l'identité du locuteur (zéro ou un), est également incluse dans f\_eGeMAPS, portant le nombre de descripteurs à 47.

Dans notre travail, les deux ensembles ont été extraits de nos données toutes les 0,25 seconde suivant le pas d'annotation d'AlloSat avec l'outil OpenEAR~\cite{OpenEAR}, qui s'appuie sur OpenSMILE~\cite{OPENSMILE}. Puisque nous ne gardons que le signal de l'appelant, nous ajoutons un descripteur, appelé \textit{vad}, pour indiquer si l'appelant parle (un) ou non (zéro). Ce descripteur est déduit des transcriptions automatiques.

Nous avons donc comparé un total de quatre ensembles :
\begin{itemize}
    \item \textbf{eGeMAPS-88} : L'ensemble de descripteur eGeMAPS, extrait avec OpenEAR sur des pas de 250~ms,
    \item \textbf{eGeMAPS-89} : Même ensemble que précédemment mais avec l'ajout du descripteur \textit{vad} qui modélise la présence ou l'absence de parole de la part du locuteur.
    \item \textbf{eGeMAPS-46} : Le sous-ensemble de descripteur utilisé par Schmitt et al.~\cite{Schmitt2019}, extrait avec OpenSMILE sur des pas de 250~ms.
    \item \textbf{eGeMAPS-47} : Même ensemble que précédemment mais avec l'ajout du descripteur \textit{vad} qui modélise la présence ou l'absence de parole de la part du locuteur.
\end{itemize}

Pour réaliser cette reconnaissance, nous utilisons deux réseaux de neurones. Afin de pouvoir comparer nos résultats avec l'état de l'art, nous avons fait le choix de reproduire le système proposé dans le challenge AVEC 2018~\cite{AVEC2018} sur la modalité \textit{Cross-cultural Affect}. Ce réseau neuronal est composé de deux couches biLSTM de respectivement 64 et 32 neurones. L'architecture bidirectionnelle est utilisée afin notamment d'éviter les problèmes de délai d'annotation. En effet, il est possible que l'annotation présente des délais, le temps que l'annotateur appuie sur les flèches du clavier ou qu'il décide s'il y a vraiment une variation à annoter. Nous avons fixé le nombre d'époque à 200.

Le second réseau est composé de quatre couches biLSTM comme décrit dans~\cite{Schmitt2019}. Les couches de biLSTM sont composées respectivement de 200, 64, 32, 32 neurones. Ce réseau est plus profond, nous avons donc augmenté le nombre d'époque à 500.

Pour ces deux réseaux, présentés dans la figure~\ref{fig:biLSTM}, la fonction d'activation utilisée est la fonction tangente hyperbolique. Un seul neurone de sortie est utilisé pour prédire une valeur toutes les 0,25 secondes.

\input{./Chapitre5/figures/biLSTM}

Les réseaux sont implémentés avec le framework Keras~\footnote{https://keras.io} en utilisant Tensorflow~\footnote{https://www.tensorflow.org/}. L'apprentissage se fait par lot de neuf conversations en utilisant l'optimiseur ADAgrad~\cite{Duchi2011} dont nous avons parlé au chapitre~\ref{chapitre2}. Les conversations sont mélangées entre chaque époque. Le learning rate est initialisé à 0,001.

Nous avons conservé les poids des réseaux donnant le meilleur score sur le développement afin de prédire les résultats sur le test.
Comme il s'agit d'une régression, le coefficient de corrélation de concordance (CCC)~\cite{CCC} a été utilisé comme fonction de coût pour l'apprentissage du réseau et comme métrique d'évaluation pour déterminer le meilleur système.
Pour rappel, ce score CCC varie de zéro (probabilité d'un tirage aléatoire) à un (corrélation parfaite).

\input{./Chapitre5/tables/result7minutes}

Le tableau~\ref{tab:result7minutes} donne un résumé des résultats obtenus avec les modèles et les ensembles de descripteurs étudiés.
Nous pouvons remarquer que l'ensemble eGeMAPS-88 et 89 nous donne de meilleurs résultats sur les deux architectures neuronales que f\_eGeMAPS-46 et 47. Nous remarquons également que l'ajout d'un descripteur \textit{vad} permet de mieux généraliser puisque les scores sur le test sont toujours meilleurs lorsque l'on a ce descripteur. De plus, l'architecture qui donne les meilleurs résultats est biLSTM-4 quelque soit l'ensemble de descripteurs utilisé.

La meilleure configuration pour ce système de reconnaissance est donc le suivant :
\begin{itemize}
  \item Ensemble de descripteur eGeMAPS-89 (avec la vad),
  \item Architecture neuronale biLSTM-4 comportant quatre couches de biLSTM.
\end{itemize}

Ces premières expériences montrent que les réseaux neuronaux biLSTM sont capables de prédire les valeurs de l'axe de satisfaction et donc de retracer cette dimension au cours d’un appel avec un score CCC correct. Néanmoins le score CCC calculé sur l'ensemble des données doit être pris avec précaution car, comme nous le montrons dans la Figure~\ref{fig:result7minutes}, le système est capable de faire de bonnes prédictions (conversation C) mais aussi de mauvaises prédictions (conversation D).

\input{./Chapitre5/figures/result7minutes}

\subsection{Décalage des annotations dans le temps}
Comme nous l'avons dit précédemment, il est tout à fait possible que les annotations soient décalées dans le temps. En effet, les annotateurs peuvent prendre plus ou moins de temps à annoter la variation d'une émotion. La question est assez complexe, puisque l'on ne connait pas la distribution des décalages. En effet, ils peuvent être très forts (décalage long) chez un annotateur ou dans une conversation en particulier, et très faible (décalage court) dans d'autres cas.

Néanmoins nous avons voulu savoir si en prenant en compte des décalages, nous sommes capables d'améliorer nos scores de reconnaissances des émotions.

Nous avons donc mis en place deux décalages : de deux secondes (soit huit pas d'annotations) et de quatre secondes (soit 16 pas d'annotations). Ce décalage à \textit{droite} consiste à décaler les annotations des segments émotionnels de $X$ pas d'annotations avec $X=8$ ou $X=16$. Les débuts de conversations, ainsi privés d'annotations, ont été remplacé par une annotation neutre de $0.50$. Les annotations \textit{trop à droite} ont été supprimées des données.

\input{./Chapitre5/tables/decalage}

Le tableau~\ref{tab:decalage} nous donne les résultats des systèmes utilisant l'architecture neuronale du biLSTM-4, présenté précédemment. Nous pouvons observer que quelque soit l'ensemble de descripteur utilisé, les scores sur les données de dev et de test sont toujours meilleurs sans décalage. Même si le décalage permet d'atteindre des scores sur l'ensemble de développement avoisinant ceux sans décalage, on remarque que le pouvoir de généralisation du modèle en pâtit grandement. Nous avons également essayer de plus forts décalages, à savoir huit et 12 secondes, mais les résultats ne font que se dégrader.

Pour notre corpus et en utilisant des architectures de type biLSTM, le décalage des annotations ne permet pas d'améliorer les scores de reconnaissance des émotions.

\subsection{Comparaison eGeMAPS et MFCC}
En reconnaissance des émotions, la plupart du temps, la représentation acoustique se concentre principalement sur la capture de la prosodie. C'est pour cette raison que les ensembles de descripteurs dits \textit{experts} sont majoritairement utilisés. En effet, comme nous l'avons dit précédemment dans le chapitre~\ref{chapitre3}, ces ensembles de descripteurs sont particulièrement adaptés dans le cadre de la recherche des émotions.

Cependant, dans notre contexte de capture des conversations, à savoir les conversations téléphoniques, le signal audio peut être plus ou moins dégradé et l'extraction de ces ensembles de descripteurs \textit{experts} peut contenir beaucoup d'erreurs. Les extracteurs ne sont, en général, pas prévus pour fonctionner avec des signaux dits dégradés.

Il peut donc être intéressant de comparer l'ensemble eGeMAPS à des descripteurs plus robustes aux signaux dégradés. Pour cela nous avons fait le choix de les comparer aux MFCC, que nous avons détaillé au chapitre~\ref{chapitre3}. Les MFCC sont plus robustes et leurs extracteurs sont plus fiables dans des contextes de signaux dégradés.

Nous avons décidé de comparer les MFCC issus de deux outils d'extraction différents : OpenSMILE dont nous avons déjà parlé et librosa\footnote{https://librosa.github.io/librosa/}. En effet, il est possible d'observer des variations entre plusieurs extracteurs~\cite{Ganchev2005}. Une fois ces extractions effectuées, nous avons mis en place deux protocoles pour les adapter à nos segments émotionnels de 250~ms :
\begin{itemize}
        \item \textbf{Mfcc-Os} correspond aux MFCC 1 à 13 ainsi qu'à leurs dérivés premières et secondes pour qualifier la dynamique du signal. Ces 39 descripteurs ont été extraits avec l’outil OpenSMILE. Pour se référer à un segment émotionnel, nous avons calculé la moyenne et l'écart type de ces descripteurs. Ainsi nous avons un ensemble de 78 descripteurs pour chaque segment émotionnel.
        \item \textbf{Mfcc-lib} correspond aux MFCC 1 à 24 qui sont extraits tous les 10~ms sur des fenêtres de 30~ms. Ces 24 descripteurs ont été extraits avec l'outil librosa. Comme pour les descripteurs précédents, nous avons calculé la moyenne et l'écart type de ces descripteurs au niveau du segment émotionnel. Ainsi nous avons un ensemble de 48 descripteurs pour chaque segment émotionnel.
    \end{itemize}

Nous utilisons la meilleure des deux architectures neuronales de la section précédente pour faire notre comparatif, à savoir le biLSTM-4 couches. Les résultats sont regroupés dans le tableau~\ref{tab:egemapsVSmfcc}.

\input{./Chapitre5/tables/egemapsVSmfcc}

Nous pouvons observer tout d'abord une forte différence de score entre les deux types de MFCC. Outre la différence d'implémentation de l'extraction, nous avons mis en place deux protocoles différents pour ces descripteurs, ce qui peut expliquer cette différence de score. On observe également que les Mfcc-lib ont une meilleure performance que l'ensemble eGeMAPS-89 sur le dev. Ce score semble confirmer que les MFCC sont plus adaptés au signal téléphonique, pour notre corpus, sur la tâche de reconnaissance des émotions.

\subsection{Comparaison CNN et biLSTM}
Dans les travaux menés par Schmitt et al.~\cite{Schmitt2019}, les architectures CNN et biLSTM sont comparées. Le postulat de ces travaux sont que nous avons trop tendance à complexifier nos architectures neuronales dans les expériences, et qu'à complexité égale, un système à base de CNN peut faire aussi bien voir mieux qu'un système à base de réseaux récurrents.

Nous avons donc voulu confronter leur conclusion à notre propre corpus, pour voir si nous pouvions améliorer nos résultats. En plus des deux architectures biLSTM précédemment expliqué, nous avons mis en place une architecture à base de quatres couches convolutionelles, avec une activation de type ReLU. Comme pour les autres systèmes, un unique neurone en sortie permet de faire la prédiction de la valeur de satisfaction. Le CNN est décrit dans la figure~\ref{fig:CNN}.

\input{Chapitre5/figures/CNN}

Afin de réaliser la comparaison, nous avons décidé de comparer nos résultats avec ceux des travaux réalisés sur le corpus SEWA. Pour cela, nous avons utilisé l'ensemble de descripteur eGeMAPS-47, utilisé par Schmitt et al.~\cite{Schmitt2019}. Nous avons donc refait leur expérimentation sur le corpus SEWA et nous l'avons adapté au corpus AlloSat, en partant du code du challenge AVEC disponible sur github\footnote{https://github.com/AudioVisualEmotionChallenge/AVEC2019}.

Nous avons eu quelques difficultés à faire converger le système convolutif sur nos données. Nous avons donc fait le choix de présenter dans le tableau~\ref{tab:cnnVSbilstm} la moyenne de cinq systèmes dont les poids ont été initialisés différemment (contrôlé par le paramètre \textit{seed}), ainsi que le meilleur des cinq systèmes sur l'ensemble de développement.

\input{./Chapitre5/tables/cnnVSbilstm}

Comme nous pouvons l'observer, nous avons un grand écart entre la moyenne des scores et le score maximal pour le CNN sur le corpus AlloSat. En effet, sur les cinq systèmes, seuls deux ont finis par converger. De plus, nous n'avons pas réussi à reproduire parfaitement les scores sur le corpus SEWA. L'initialisation des systèmes joue un très grand rôle dans le score final. On remarque également que les scores atteints avec une architecture biLSTM-4 sur l'axe de satisfaction est comparable aux scores obtenus sur les axes d'activation et de valence de SEWA.

Pour conclure, nous avons montré que les systèmes à base de couches convolutives ne sont pas compatibles avec notre corpus. De plus, nous voulons à terme travailler sur des séquences de taille non fixe, ce qui ne peut pas être réalisé avec des CNN.

\section{En utilisant l'intégralité des conversations}
D'après ce que nous avons observé sur les extraits de conversations, nous avons décidé de faire des expérimentations sur la totalité des conversations en utilisant les différentes configurations expliquées plus tôt. Nous ne pouvons pas faire d'expérimentations à partir de systèmes à base de CNN, dû à la nature de ce réseau, qui permet de faire des reconnaissances à partir de séquences de taille fixe.

Nous avons également choisi de changer d'implémentation en passant de l'utilisation de Keras à celle de Pytorch~\cite{pytorch}. En effet cet outil permet de gérer plus en profondeur les paramètres des réseaux et a un temps d'apprentissage fortement diminué.

\subsection{Changer de fonction de coût}
Comme nous l'avons vu précédemment dans le chapitre~\ref{chapitre3}, le Coefficient de Corrélation de Concordance est un outil d'évaluation très utilisé dans la reconnaissance d'émotions continues. Les challenges AVEC~\cite{AVEC2018} ont notamment contribué à la standardisation de l'utilisation de cette métrique pour évaluer les systèmes de reconnaissance d'émotion continue. Pour rappel, le CCC se calcule selon l'équation~\ref{eq:CCCscore}.

\begin{equation}
   \rho = \frac{2\rho\sigma_x\sigma_y}{\sigma_x^2 + \sigma_y^2 + (\mu_x - \mu_y)^2}
\label{eq:CCCscore}
\end{equation}

Si on regarde l'équation, quand la référence est constante sur la conversation totale, $\sigma_y = 0$ et donc le coefficient est égal à zéro. De façon plus général, quand la référence varie peu, le CCC va s'approcher de zéro, même si la prédiction est quasiment parfaitement synchronisée à la référence.

On peut donc en conclure que la fonction de coût va pénaliser les conversations où la référence varie peu ($\sigma_y \simeq 0$) et que le système ainsi entraîné aura du mal à prédire correctement de tels références.

\input{./Chapitre5/tables/cccVSrmse}

Nous avons comparé les scores obtenus en utilisant la RMSE comme fonction de coût que nous avons introduit au chapitre~\ref{chapitre3} pour palier à ce problème. Le tableau~\ref{tab:cccVSrmse} résume les différences de scores calculés sur le dev.

Comme nous pouvons l'observer, les scores sont majoritairement meilleurs si on utilise la fonction de coût CCC. On remarque cependant que dans le cas des descripteurs \textit{Mfcc-lib}, l'utilisation de la fonction de coût RMSE permet d'améliorer les résultats. Cependant lorsque l'on regarde la prédiction effective sur les conversations, on se rend compte que les prédictions ne sont pas homogènes en terme de qualité, comme en témoigne la figure~\ref{fig:cccVSrmse}.

\input{./Chapitre5/figures/cccVSrmse}

Nous avons donc décidé de mettre en place un post-traitement afin de lisser les courbes de prédiction, pour avoir une meilleure représentation des émotions.

\subsection{Post-traitement : lissage des prédictions}
Nous avons décidé de mettre en place un lissage des prédictions. Pour cela, nous avons utilisé l'algorithme de lissage Savistky-Golay~\cite{Savitzky1964} avec un degré polynomial de zéro. Bien qu'ancien, cet algorithme est toujours autant utilisé dans la gestion des signaux. C'est une extension de la moyenne glissante, qui est utilisée pour atténuer les pics des signaux en approximant un polynôme pour chaque fenêtre de la moyenne glissante, ici un polynôme de degré zéro. Nous avons fait le choix de ce degré puisque c'est celui qui va lisser au maximum la courbe.
%It has been applied to the series of data with a window of 101, again, to try flatten the curve as much as possible.

Nous avons donc appliqué ce lissage sur les prédiction du meilleur système obtenu jusque là, les résultats sont disponibles dans le tableau~\ref{tab:lissage}.

\input{./Chapitre5/tables/lissage}

Nous pouvons observer qu'il s'agit du meilleur score jamais obtenu sur les prédictions de l'axe de satisfaction. De même, la figure~\ref{fig:lissage} permet de se rendre compte du profit que nous tirons du lissage.

\input{./Chapitre5/figures/lissage}

Grâce à ce lissage, nous obtenons de très bons scores : $0.719$ pour l'ensemble de développement et $0.570$ pour le test. Nous avons voulu confronter ces scores à ceux obtenus dans l'état de l'art, sur le corpus SEWA.

\subsection{Comparaison avec SEWA}
Pour comparer nos résultats avec ceux obtenus sur le corpus SEWA, nous nous sommes remis dans les conditions d'expérimentation des campagnes AVEC 2018 et 2019. Nous avons également comparés nos résultats avec ceux obtenus par Schmitt et al.

Tout d'abord nous avons repris les systèmes décrits dans ces expérimentations pour reproduire les résultats reportés dans les différents articles:

\begin{itemize}
  \item biLSTM-2 : comme montré dans la figure~\ref{fig:biLSTM}, il s'agit d'un système à deux couches de biLSTM de 64 et 32 neurones. Un seul neurone est utilisé pour la sortie. Ce système est utilisé comme baseline dans les campagnes AVEC~\cite{AVEC2018,AVEC2019} en 2018 et 2019 pour la sous-catégorie \textit{cross-cultural affect}. Il est constitué de 337 473 paramètres.
  \item biLSTM-4 : comme montré dans la figure~\ref{fig:biLSTM}, il s'agit d'un système à quatre couches de biLSTM de 200, 64, 32 et 32 neurones. Un seul neurone est utilisé pour la sortie. Ce système est utilisé dans les travaux de Schmitt et al.~\cite{Schmitt2019}. Il est constitué de 702 593 paramètres.
\end{itemize}

\input{./Chapitre5/tables/allosatVSsewa}

Les résultats sont disponibles dans le tableau~\ref{tab:allosatVSsewa}. L'initialisation étant un facteur important dans le score final des systèmes, nous avons fait le choix de présenter à la fois la moyenne de cinq systèmes initialisés aléatoirement et le meilleur de ces cinq systèmes. Comme nous pouvons l'observer, les résultats de nos modèles sont comparables à ceux obtenus dans les différentes baselines sur le corpus SEWA. Ils sont meilleurs que ceux diffusés lors de la campagne de 2018 mais moins performant que ceux de la campagne 2019. Les auteurs de la campagne ont eux-même reconnus que des variations de scores peuvent être observées en fonction de l'initialisation.

On remarque également que nous n'avons pas réussi à bien reproduire les résultats obtenus par Schmitt et al. sur l'activation et la valence. Ceci peut être dû notamment à l'initialisation mais également à la gestion du délai des annotations qui n'est pas pris en compte dans nos expérimentations. Nos systèmes sont plus performants sur la dimension du \textit{liking}, quelque soit les architectures utilisées.

Nous observons également, en comparant les meilleures performances et les performances moyennes, que le système biLSTM-2 entraîné avec les descripteurs Mfcc-Os sur les conversations allemandes et hongroises (en violet) semble être le moins sensible à l'initialisation des poids. De façon général, la plus grande disparité est observée sur la dimension du \textit{liking}.

Les performances obtenues sur l'axe de satisfaction sont comparables à celles obtenues sur les autres dimensions dans le cas de systèmes à base de réseaux récurrents. Les réseaux convolutifs, comme nous l'avons énoncé auparavant, ne semblent pas convenir à notre corpus. La prédiction de l'axe de satisfaction est plus performante avec des réseaux récurrents que des réseaux convolutifs. Il est donc plus prudent d'utiliser des réseaux récurrents lors de tâches de SER, puisqu'ils semblent plus robustes aux changements d'axe d'émotion et aux changements de corpus. De plus, la convergence des systèmes convolutifs n'est pas assurée en fonction des différents paramètres et hyper-paramètres du système.

En se concentrant sur la prédiction de l'axe de satisfaction, eGeMAPS-88 ($ccc=0.480$) semble mieux fonctionner que Mfcc-Os ($ccc=0.364$) avec le système biLSTM-2. Cependant, en utilisant le système biLSTM-4, les descripteurs eGeMAPS-47 atteignent également de bons résultats ($ccc=0.437$). Pour conclure sur le meilleur nombre de couches, nous effectuons une dernière expérience avec le système biLSTM-4 et eGeMAPS-88 ($ccc=0.564$) qui obtient notre meilleur résultat. Cette architecture semble être la plus convenable pour l'axe de satisfaction.

De cette expérience, nous concluons que le système biLSTM-4 est la meilleure des trois architectures comparées en ce qui concerne la robustesse à la variabilité induite par l'utilisation de différents corpus d'émotions et d'axe d'émotions. Nous cherchons les différences entre les deux corpus qui pourraient expliquer la différence de performance entre les systèmes sur ces deux corpus.

\subsection{Analyse des différences entre SEWA et AlloSat}

\subsubsection{Variation de l'annotation : pas de l'annotation, taux d'échantillonnage}
Nos premières hypothèses ont porté sur la durée du pas d'annotation et sur la différence de taux d'échantillonnage entre les deux corpus. Nous avons donc décidé de changer les pas d'annotation des deux corpus pour qu'ils soient comparables. Comme nous avons un corpus annoté toutes les 100~ms et l'autre annoté toutes les 250~ms, nous avons choisi de considérer l'annotation toutes les 500~ms. Pour cela, nous avons utilisé une moyenne glissante, afin de rester cohérent avec les annotations d'origines. Nous avons dans le même temps sur-échantillonné les conversations issues d'AlloSat, afin de se ramener à deux corpus échantillonnés selon le même taux soit 44kHz.

\input{./Chapitre5/tables/pasAnnotation}

Les résultats, présentés sur le tableau~\ref{tab:pasAnnotation} ne permettent pas de valider ces deux hypothèses. En effet, les résultats sont comparables aux précédents.

\subsubsection{Variation de l'annotation : intensité de la dynamique}
En comparant les deux corpus, nous remarquons que l'axe de satisfaction varie très lentement dans le temps par rapport à l'activation et à la valence. Cela peut être dû au protocole d'annotation (la souris ou le joystick) et au contenu affectif annoté. Cette différence de variation peut être observée sur la figure~\ref{fig:variationAnnot} où la dynamique a été quantifiée en utilisant la ZCR sur les deltas des dimensions. Cette différence de variation pourrait expliquer la différence d'échelle entre les scores obtenus sur le corpus AlloSat et le corpus SEWA.

\input{./Chapitre5/figures/variationAnnot}

Pour déterminer si la dynamique dans l'annotation est responsable des mauvais résultats des systèmes convolutifs sur la satisfaction, nous effectuons des expériences supplémentaires avec des références lissées pour l'activation et la valence. Pour mettre en place le lissage, nous avons utilisé une moyenne glissante sur des fenêtres de 200, 500~ms et une seconde sur SEWA, ainsi que de 500~ms et une seconde sur AlloSat. Les résultats, rapportés dans le tableau~\ref{tab:allosatLisse} montrent qu'aucun de ces procédés ne permettent d'améliorer les performances des systèmes.

\input{./Chapitre5/tables/allosatLisse}

On peut en conclure que la dynamique faible de l'annotation présente dans le corpus AlloSat n'est pas responsable de la faible performance du système à base de réseaux convolutionels.

\subsubsection{Impact du bruit téléphonique}
Une autre piste que nous avons suivi pour expliquer la dégradation des scores d'AlloSat provient de la modalité téléphonique.
En classification d'images, des études~\cite{Roy2018,Dodge2016} ont montré que l'entraînement de modèles à base de CNN avec des images de mauvaise qualité peut considérablement dégrader les performances des modèles de classification d'images.
Comme AlloSat est composé de conversations téléphoniques, l'enregistrement audio est échantillonné en huit kHz et il est rempli de bruits : bruit de fond, coupures de son, changement de haut-parleur, etc. Nous émettons donc l'hypothèse qu'AlloSat est trop dégradé pour être bien traité par des réseaux convolutifs.
Pour confirmer notre hypothèse, nous avons sous-échantillonné les données SEWA de 44 kHz à huit kHz.

Nous proposons également d'ajouter du bruit au jeu de données SEWA, afin de le rendre plus comparable avec le contexte AlloSat. Les bruits d'appels téléphoniques sont principalement composés de bruits de fond tels que les bruits de la rue ou de la circulation.

Pour injecter du bruit dans nos enregistrements, nous avons utilisé la base de données SoundJay~\footnote{https://www.soundjay.com/} et nous avons suivi le processus décrit dans l'article~\cite{Alzqhoul2016}. Trois types de bruits (voiture, rue et conversations de foule) sont ajoutés de manière aléatoire au signal à différents niveaux de volumes (9, 15, 21 décibels). En plus de ces bruits, nous avons fait le choix d'ajouter des voix d'enfants, puisque l'on en entend assez souvent dans le corpus. Les enregistrements résultants ont été vérifiés manuellement pour vérifier si les perturbations auditives sont comparables à celles observées dans AlloSat.

\input{./Chapitre5/tables/downsample}

Les résultats, présentés dans le tableau~\ref{tab:downsample}, montrent qu'en dégradant les signaux SEWA en sous-échantillonnant à 8 kHz et en ajoutant du bruit, nous diminuons les performances. Mais nous sommes loin d'atteindre les performances des réseaux convolutifs sur AlloSat.

Toutes nos hypothèses ne nous permettent pas de justifier la différence de score entre AlloSat et SEWA lorsque l'on utilise des réseaux convolutifs. Le changement de la dimension annotée, les conditions d'enregistrement, le contenu sémantique, la langue ou encore le protocole mis en oeuvre sont d'autant de justificatif possible quant à la différence de performance de ces réseaux sur les deux corpus.

\section{Conclusion}
Dans ce chapitre, nous avons décrit les différents choix que nous avons considérer pour construire nos premiers systèmes de reconnaissance des émotions. Nous avons d'abord parlé de la reconnaissance d'émotions discrètes puis continues. Nous avons également controler nos expérimentations et la pertinence de notre corpus en le comparant au corpus de l'état de l'art SEWA.

Nous pouvons conclure que le meilleur système de reconnaissance des émotions continues utilise un réseau de neurones récurrents, un ensemble de descripteur tiré des MFCC, sur lequel nous appliquons un post-traitement de lissage.

Cependant comme nous l'avons dit précédemment, la modalité acoustique n'est pas la seule permettant de retrouver les états émotionnels. Dans le prochain chapitre, nous allons mesurer l'apport de la modalité linguistique ainsi que l'apport de descripteurs pré-entrainés de type BERT, alors nouveau.
