\chapter{Reconnaissance automatique des émotions}
\label{chapitre3}
«Si vous voulez être libre de vos émotions il faut avoir la connaissance réelle, immédiate de vos émotions.»  (Arnaud Desjardins, 1925-2011)


\section{Définition du Speech Emotion Recognition}
Le domaine du Speech Emotion Recognition se concentre sur les tâches de reconnaissance des différents états émotionnels d'un ou de plusieurs locuteurs. Ce domaine est en plein expansion grâce notamment à l'utilisation de nouveaux systèmes neuronaux empruntés à d'autres domaines du Machine Learning.
Afin de mettre en place des expérimentations sur ces tâches de reconnaissances et de caractérisations des émotions, il est important de mettre en place des données pertinentes sur lesquelles s'appuyer, des méthodes pour représenter efficacement ces données ainsi que de mettre en place des systèmes d'évaluation robustes permettant de comparer les différentes expérimentations.

\section{Les Corpus existants}
Aujourd'hui, les tâches de reconnaissance d'émotion sont principalement traitées en tant que tâches supervisées : le système apprend à reconnaître des étiquettes émotionnelles associées à des segments de parole à partir de corpus de parole annotés en émotions.

\subsection{Les différents types de corpus}
\subsubsection{Corpus actés}
Comme nous l'avons vu dans le premier chapitre de cette thèse, la caractérisation en émotion d'une personne peut être assez délicate, puisqu'elle est en grande partie subjective. De plus, il est communément admis que les états émotionnels sont ponctuels et rares dans la parole. Pour palier à ce problème, on construit des corpus dit \textit{actés}. Il s'agit de corpus où l'état émotionnel n'est pas naturel.

On fait alors appel à des acteurs, qui vont simuler des états émotionnels dictés par le responsable de l'élaboration du corpus. Par exemple, les acteurs devront dire le mot \textit{hippopotame} de façon joyeuse, triste puis avec dégoût. Cette pratique est très utilisée pour avoir différents états émotionnels d'un même locuteur, le tout rapidement. Elle est à privilégier quand on recherche l'exhaustivité des états émotionnels chez un sujet. Ces corpus sont aujourd'hui majoritaires. On peut notamment citer les corpus Emo-Db~\cite{Burkhardt2005} et DES~\cite{Engberg1997}. Les inconvénients d'un tel protocole sont que les émotions obtenues sont prototypiques dans leur manifestation, les rendant difficile à comparer à des émotions dites \textit{naturelles}. De plus, l'utilisation d'acteurs peut se révéler coûteuse.

\subsubsection{Corpus induits}
Un autre type de corpus utilisé dans la reconnaissance d'émotion est le corpus induits. On utilise différentes méthodes pour induire les états émotionnels que l'on souhaite étudier.

Une des méthodes pour construire ce genre de corpus correspond au \textit{magicien d'Oz} abrégé WoZ (\textit{Wizard of Oz}). Cette méthode consiste à simuler un dialogue homme-machine en changeant la machine par un opérateur humain, qui va simuler une réponse machine. Ainsi l'humain pense avoir affaire à un serveur vocal, alors qu'un opérateur est responsable des réponses engendrées par le système.

On peut également parler de la recherche de stress par laquelle on peut présenter aux sujets des situations stressantes comme des montagnes russes~\cite{Hansen1997} ou une prise de parole en public~\cite{Giraud2013}. Quant à la tristesse, on peut utiliser des extraits de films par exemple~\cite{Schuller2010cinemo}. Ces émotions sont plus proches des émotions réelles, elles sont donc moins manifestes.

Les corpus actés et induits ont pour avantage d'être créer dans des environnements contrôlés et permettent de rendre les données facilement accessibles. Ces données sont, la plupart du temps, produites à des fins d'analyse. Il est donc facile de recueillir le consentement des participants avant la mise en place de l'expérience, de contrôler la qualité des enregistrements et la cohérence des données en contraignant les participants sur un sujet ou à exprimer un type d'état émotionnel défini. Néanmoins ils ne représentent pas la vie réelle, sont souvent de petite taille et sont difficilement fusionables pour permettre d'avoir de plus grande quantité de données.

\subsubsection{Corpus naturels}
Les corpus non actés, aussi appelés naturels ou \textit{real-life} sont obtenus directement à partir de données issues de la vraie vie. Ils proviennent d'environnements moins contrôlés où les participants n'ont pas connaissance de leur implication dans l'expérimentation a priori. On peut travailler par exemple avec des enregistrements provenant de débats télévisés, de reportages, de vidéos internet ou de centre d'appels par exemple. Ces données sont donc constituées d'états émotionnels spontanés. Ces derniers sont moins marqués et prototypiques que des émotions actées. De plus la parole spontanée n'est pas forcément marquée par des états émotionnels.

Ces corpus sont difficile à construire et à diffuser : soit les participants doivent être retrouvés et ils doivent donner leurs consentement a postériori, soit les données doivent être anonymisées pour respecter la réglementation générale sur la protection de données. De même, il est difficile d'avoir tous les états émotionnels de tous les locuteurs et de garantir une qualité d'enregistrement identique entre tous les documents.
De plus, l'étiquetage de ces données est souvent plus complexe, puisque le cadre expérimental n'a pas été expliqué aux participants.

\subsection{Les différents types d'acquisition}
L'acquisition des enregistrements est un paramètre important dans la catégorisation des corpus. En effet, un ensemble d'enregistrement capté par un micro spécifique ne donnera pas la même qualité acoustique qu'une voix téléphonique.

On retrouve plusieurs types d'acquisition :
\begin{itemize}
  \item Acquisition en labo : le milieu d'acquisition est contrôlé par les responsables du corpus. Cela permet une homogénéité entre les différents enregistrements. On peut notamment citer le corpus SEWA~\cite{SEWA}.
  \item Acquisition en studio : lorsque l'on travaille sur des données déjà acquise par des studios de télévision par exemple. MSP-Podcast\cite{Lotfian2019} regroupe des enregistrements de podcast, réalisé en studio.
  \item Acquisition en condition réelle : le milieu d'acquisition est peu contrôlé par les responsables du corpus. Ce genre d'acquisition est prépondérantes dans les corpus naturels. On peut citer notamment les corpus comportant des conversations de centres d'appels comme CallSurf~\cite{Garnier2008} ou Natural~\cite{Morrison2007} ou de micro trottoir ou d'enregistrement de conversations dans des lieux public comme ESLO~\cite{Eshkol2011}.
\end{itemize}

\subsection{Les différents types d'annotation}
Comme nous l'avons vu précédemment, l'annotation des données émotionnelles peut se faire de différentes manières. On se concentre principalement sur l'annotation discrète et l'annotation continue en émotion. Ces corpus sont donc généralement annotés selon une de ces deux méthodes, en respectant un schéma d'annotation qui lui est propre, ce qui ne facilite pas l'utilisation de plusieurs corpus pour construire des systèmes de reconnaissance d'émotion.

Pour garantir la qualité de l'annotation, plusieurs indicateurs sont à notre disposition, notamment le kappa ou le coefficient de corrélation. Ces deux méthodes sont décrites dans le prochain chapitre.

\subsection{Synthèse}

\input{./Chapitre3/tables/corpus}

Les principaux corpus actés et non actés, utilisés dans la reconnaissance automatique d'émotion, sont listés dans le tableau~\ref{tab:corpus}.

Les corpus les plus utilisés de nos jours sont le Berlin Emotional database, aussi appelé EMO-DB et IEMOCAP. Il s'agit de deux corpus annotés selon des catégories discrètes. EMO-DB~\cite{Burkhardt2005} est composé de phrases courtes allemandes prononcées par 10 acteurs et est annoté en peur, colère, joie, tristesse, dégoût, ennui et neutre. IEMOCAP~\cite{Busso2007} est composé de conversations scriptées entre deux acteurs et est annoté en peur, colère, joie, tristesse, dégoût, frustration, surprise, excitation, neutre et \textit{autres} pour toutes les autres émotions. Joué par 10 acteurs également, cette base de données est composée de 12 heures d'audio, ce qui lui permet d'être compatible avec des approches neuronales profondes, bien qu'on préfère généralement travailler avec un plus gros volume de données.

Nous pouvons également citer en particulier le corpus RECOLA et le corpus SEWA. Ces deux corpus sont particulièrement adaptés à notre tâche, puisqu'ils sont annotés selon des émotions continues.
RECOLA~\cite{Ringeval2013} est constitué de conversations dyadiques effectuées en visioconférence pendant laquelle les deux participants doivent compléter une tâche qui leur demande de coopérer. La base de données est notamment constituée des 5 premières minutes de l'enregistrement audio des 23 binômes ainsi formés, totalisant 3 heures et 50 minutes d'audio. 6 annotateurs ont mesurés les états de valence et d'activation des participants.
SEWA~\cite{SEWA} est constitué de conversations entre deux locuteurs concernant des publicités visualisées en amont. Le corpus est notamment constitué des enregistrements audio de ces conversations réalisées en 6 langues différentes et réunissant 398 participants pour un total de 44 heures. Ces deux corpus sont disponibles pour les membres d'institution de recherche, en faisant des corpus de plus en plus utilisés.

Dans le cadre de cette thèse, nous avons décidé de comparer nos résultats à ceux obtenus avec le corpus SEWA, comme il se rapproche sur plusieurs points de notre problématique.

\section{Les descripteurs}
\subsection{Descripteurs acoustiques}
Afin de valoriser les données que nous avons dans les différents corpus, il est important de mettre en place une transformation pertinente des données brutes en features, aussi appelés caractéristiques ou descripteurs en français. Les descripteurs proviennent des domaines de la phonétique notamment et de la reconnaissance de la parole ou de la musique. Leur but est d'obtenir les caractéristiques phonatoires et articulatoires des locuteurs~\cite{Scherer1986} afin d'extraire les informations linguistiques et para-linguistiques du discours.

\subsubsection{Spectrogramme}
%\input{./Chapitre3/figures/spectrogramme}

Un spectrogramme est une représentation du signal audio mettant en avant l'énergie en fonction de la fréquence et du temps. %Comme on peut le voir sur la figure~\ref{fig:spectrogramme},
Il s'agit d'une représentation tridimensionnelle du signal audio. Cette transformation est possible en utilisant la transformation de Fourier qui permet de passer de l'espace temporel à l'espace fréquentiel. Les zones les plus chaudes (jaune/orange) de la figure correspondent aux énergies les plus fortes, que l'on appelle des formants.

Cette représentation est souvent utilisé lorsque l'on considère le signal audio comme une image. On est ainsi capable d'utiliser des systèmes de reconnaissance d'images afin de traiter des problématiques touchant au domaine de la parole~\cite{Stolar2017}.


\subsubsection{Coefficients Cepstraux de Fréquence de Mel}
Les MFCCs, Mel-Frequency Cepstral Coefficients en anglais, sont des descripteurs spectraux utilisés très largement en traitement du signal audio, que ce soit en reconnaissance de la parole, en identification du locuteur ou en détection de concepts sémantiques par exemple. Ils sont issus d'un ensemble de traitements qui est appliqué sur le signal audio traditionnellement sur des fenêtres temporelles de 30 ms tous les 10 ms. Afin de modéliser l'évolution temporelle du signal, on utilise les dérivées premières et secondes de ces coefficients. Contrairement aux Linear Predictive Coding (LPC)~\cite{Rabiner1993},
%ou les Perceptual Linear Prediction (PLP)~\cite{Hermansky1990},
ces coefficients perceptifs sont adaptés à l'audition humaine puisqu'ils suivent l'échelle de perception de Mel. En effet, notre perception des sons n'est pas linéaire : nous percevons plus de différence entre des sons de 1000 et 2000 kHz qu'entre des sons de 7000 et 8000 kHz.

Robuste au bruit, ces coefficients permettent de représenter le spectre de façon compacte et en éliminant les redondances que l'on trouve dans le signal brut.

%Au total, nous avons donc une représentation en vecteurs de taille 39 pour chaque fenêtre de 10ms de signal, nous permettant de réduire considérablement le signal d'entrée, tout en gardant les informations essentielles contenues dans l'audio.

\subsubsection{Descripteurs prosodiques}
Dans le domaine du Speech Emotion Recognition, il n’y a pas de consensus sur le meilleur ensemble de descripteurs à utiliser pour effectuer des tâches de reconnaissance d'émotions. C'est pour cela qu'il existe un grand nombre de set de descripteurs regroupant les différents indices acoustiques mis en relation avec les émotions. Ces indices sont soit extraits au niveau de fenêtres de courtes durées (30~ms le plus souvent), soit sur des fenêtres plus longues (plus d'une seconde) pour capturer les phénomènes para-linguistiques.

Traditionnellement, on prend un très grand nombre de descripteurs, comme par exemple le set de base extrait avec OpenSMILE~\cite{OPENSMILE}, qui compte 988 descripteurs acoustiques, puis on les filtre pour ne conserver que les plus pertinents par une sélection de descripteurs.

La sélection des descripteurs réduit la dimensionnalité de leur espace, supprime les données redondantes, non pertinentes ou bruyantes. Elle apporte des effets bénéfices directes aux systèmes : l'accélération du temps de traitement puisqu'il y a moins de données à traiter, l'amélioration de la qualité des données et donc de la performance des systèmes. De plus, elle peut permettre de rendre les résultats plus compréhensibles.
Cette sélection peut s'effectuer de plusieurs manières :
\begin{itemize}
    \item selon des méthodes de ranking, en comparant les descripteurs les uns aux autres.
    \item en utilisant des \textit{wrappers} : en utilisant le modèle de classification et en testant chaque descripteur les uns après les autres. Pour ce faire, on les enlève ou on les ajoute un à un et on sélectionne l'ensemble de descripteurs qui donne au modèle sa meilleure performance.
\end{itemize}
Néanmoins l'ordre de grandeur des corpus annotés en émotion peut vite poser problème si la dimension des descripteurs est trop importante.

Pour palier à ce problème, de plus petits ensembles de descripteurs, réalisés par des experts du domaine ont été proposés. On peut notamment citer l'ensemble INTERSPEECH 2009~\cite{Schuller2009} ainsi que \textit{The Geneva Minimalistic Acoustic Parameter Set} (GeMAPS) et sa version étendue (eGeMAPS)~\cite{Eyben2016}.

L'ensemble INTERSPEECH 2009 contient 384 descripteurs qui ont été utilisés comme référence lors du challenge de 2009. Il est composé de 16 descripteurs de bas niveau (Low Level Descriptors en anglais) :
\begin{itemize}
  \item \textit{zero crossing rate} : taux de passage par 0 du signal sur une fenêtre temporelle donnée.
  \item \textit{RMS energy} ou énergie moyenne quadratique : modélise la variation de l’énergie du signal à chaque fenêtre d'analyse.
  \item \textit{F0} ou fréquence fondamentale.
  \item \textit{Harmonic to noise ratio} ou rapport Harmonique-Bruit : modélise le bruit contenu dans le signal.
  \item \textit{MFCC} : les 12 premiers coefficients MFCCs sont utilisés.
\end{itemize}
Sur ces descripteurs sont calculées 12 fonctionnelles (la moyenne, la variance, le coefficient de Pearson, de dissymétrie, le maximum, le minimum, la position relative, la plage de valeur et la régression linéaire). 16 descripteurs et leurs 16 dérivées sur lequel on applique 12 fonctions pour un total de 384 descripteurs.

GeMAPS est composé de 18 descripteurs de bas niveau représentant des propriétés de fréquence, d’énergie, d’amplitude et des propriétés spectrales sur lesquelles sont appliquées des fonctions statistiques pour un total de 62 descripteurs. Nous reprenons la liste établie par l'article de Eyben et al.~\cite{Eyben2016} dans le tableau~\ref{tab:egemaps}.

\input{./Chapitre3/tables/egemaps}

Comme la version courte de l’ensemble minimaliste ne contient aucun paramètre cepstral et très peu de paramètres dynamiques, on ajoute sept LLD pour construire l’ensemble d’extension. En lui appliquant des fonctions statistiques, eGeMAPS contient un total de 88 descripteurs. Le tableau~\ref{tab:egemaps} résume toutes les caractéristiques retenues dans ces deux ensembles.

\subsubsection{Bag-of-Audio-Words}
Comme nous l'avons déjà indiqué, la sélection de la représentation de l'audio est un choix qui va directement influencer la qualité de la reconnaissance des émotions. C'est pour cela qu'il existe de nombreuses représentations, dont les sacs de mots-audio, ou BoAW. Inspiré des sacs de mots utilisés en NLP, il s'agit d'utiliser des LLDs sélectionnés pour former un lexique de toutes les valeurs possibles, puis de les coder par un vecteur. Ce vecteur est alors utilisé en tant qu'entrée du système.

Cette solution présente comme avantage de renforcer la robustesse du système, vu que les LLDs en entrées sont en quelque sorte normalisées par ce processus. Ces features sont utilisées dans de nombreuses tâches reliées à la parole : la classification d'évènements sonores ou la détection de musique par exemple. Ils ont également été utilisés en reconnaissance d'émotion continue.
%In this approach, feature vectors of acoustic LLDs are quantised according to a learnt codebook of audio words. Then, a histogram of the occurring ‘words’ is built.

\subsection{Descripteurs linguistiques}
Nous avons vu que lorsque l'on cherche à déterminer l'état émotionnel d'un locuteur à partir d'un enregistrement, l'approche la plus naturelle consiste à extraire des descripteurs acoustiques directement à partir du signal. On peut cependant ajouter des informations linguistiques, syntaxiques, phonétiques et sémantiques. Ces informations peuvent être extraites directement à partir d'une transcription automatique de la parole.

Les domaines du Sentiment Analysis et de l'Opinion Mining cherchent à identifier une opinion à partir d'un texte écrit. Il y a ici une différence significative entre opinion et état émotionnel, cependant les approches peuvent se rejoindre. On peut également considérer la transcription automatique comme un élément associé à la parole, et donc y retrouver des marqueurs de l'émotion. Nous détaillons dans cette partie certaines méthodes utilisées dans ces domaines pour transformer du texte (dans notre cas la transcription automatique) en descripteurs.

\subsubsection{Représentation en one-hot}
La représentation en \textit{one-hot}, comme illustrée sur la figure~\ref{fig:onehot}, correspond à associer à chaque mot un vecteur de binaire d'une taille fixe. On rassemble tous les mots différents utilisés dans ce que l'on nomme un vocabulaire. Puis pour chaque entrée (appelé token) de ce vocabulaire, on associe un vecteur binaire unique. Cette méthode présente l'avantage d'être exhaustive et facile à mettre en œuvre, cependant la représentation est très volumineuse : le vecteur sera de la taille du vocabulaire et elle contiendra principalement des zéros.

\input{./Chapitre3/figures/onehot}

Cette représentation permet de réaliser des opérations numériques directement sur les vecteurs \textit{one-hot}. Cependant elle ne porte aucune information sur le contexte (la position du token dans la séquence), la sémantique ou sur le nombre d’occurrence de chaque token.

\subsubsection{Représentation statistique}
Parmi les représentations statistiques des données, les plus courantes et les plus mises en place sont les méthodes TF (Term Frequency) et TF-IDF (Term Frequency-Inverse Document Frequency) qui prennent en compte la distributions des tokens dans les documents. Pour TF, on prend le nombre d’occurrence d'un token divisé par le nombre de termes dans le document. Pour TF-IDF, on divise par l'inverse de la fréquence dans tous les documents, comme présenté dans les équations~\ref{eq:tfidf},~\ref{eq:tf},~\ref{eq:idf} avec $t$ le token, $d$ le document et $n$ le nombre de documents. Ainsi, les termes qui sont trop fréquents ou présents dans tous les documents sont moins mis en avant par la représentation.

\begin{equation}
    TF-IDF = TF(t,d) x IDF(t)
\label{eq:tfidf}
\end{equation}

\begin{equation}
    TF = \dfrac{Nombre de t dans d}{Nombre de mots dans d}
\label{eq:tf}
\end{equation}

\begin{equation}
    TF = log(\dfrac{N}{(Nombre de t dans tous les documents) + 1})
\label{eq:idf}
\end{equation}

Ces méthodes statistiques ont fait leur preuve~\cite{Martineau2009,Cambria2013,Pimpalkar2020}, même si on leur préfère maintenant des méthodes qui intègrent des aspects sémantiques notamment.

\subsubsection{Plongement de mots}
\input{./Chapitre3/figures/word2vec}
\textcolor{red}{pas satisfaite de cette partie}
Les plongements de mots (word embeddings en anglais) ont été présentés par Bengio et al.~\cite{Bengio2003}. Ils décrivent une nouvelle représentation des mots basée sur une approche neuronale afin de construire des modèle de langues.
On projette les mots dans un espace de faible dimension, tout en isolant ensemble les mots qui ont des similarités sémantiques et syntaxiques~\cite{Ghannay2017}. Cela permet d'associer à chaque token un vecteur de valeurs réelles, denses d'une dimension bien inférieure à celle utilisée pour la représentation \textit{one-hot}. Chaque vecteur est ensuite inscrit dans un dictionnaire, et on peut alors remplacer chaque mot par le vecteur le représentant.

Leur utilisation et leur pertinence a été démontré dans de nombreuses tâches, notamment des tâches de TALN: l’étiquetage morphosyntaxique, la reconnaissance d’entités nommées, la détection de mention~\cite{Turian2010,Bansal2014} et de compréhension de la parole~\cite{Mesnil2013,Yao2014,Liu2016}.

Parmi les méthodes les plus utilisées on retrouve Word2vec qui peut s'apprendre avec deux algorithmes (CBOW et Skip-gram)~\cite{word2vec} ou encore GloVe~\cite{Pennington2014}.

Il est utile de noter que ce genre de représentation peuvent être visualisé dans un espace plus restreint, typiquement en deux dimensions. Ainsi les expérimentateurs peuvent observer les rapprochements sémantiques ou syntaxiques détectés par le système, comme l'illustre la figure~\ref{fig:word2vec}.

\section{Évaluation des performances}
De nombreuses métriques ont été utilisées au fur et à mesure de l'avancée du domaine pour évaluer les performances des systèmes de reconnaissance d'émotions dans la parole suivant le type de tâche : classification ou régression.

Pour les tâches de classification les métriques d'accuracy (précision) et de rappel moyen(ne) pondéré ou non pondéré sont les métriques les plus utilisées traditionnellement. On peut notamment se référer aux challenges INTERSPEECH en émotions de 2009 et 2011~\cite{Schuller2009,Schuller2011} qui utilisent le rappel moyen non pondéré comme métrique.

En ce qui concerne les tâches de régression, l'erreur quadratique moyenne est une métrique performante et souvent utilisée avant que des campagnes d'évaluation mettent en place le Coefficient de Corrélation de Concordance (CCC) comme la mesure la plus usitée dans l'évaluation de systèmes de reconnaissance de l'émotion continue.

\subsection{Tâche de classification : Matrice de confusion et scores associés}
La matrice de confusion est une matrice permettant de mesurer la performance d'un système de classification. Pour rappel, à chaque segment émotionnel est associée une classe qui définit l'état émotionnel de la personne. Cette matrice est donc mise en place en tant qu'évaluation lorsque les émotions sont de nature discrètes. Grâce à elle, on peut retrouver les différentes erreurs du système et les quantifier. Un exemple de matrice de confusion est donné par le tableau~\ref{tab:matriceConf}. Les lignes correspondent aux références, et les colonnes aux prédictions d'un système.
\textcolor{red}{arranger le tableau, les lignes font moches}
\input{./Chapitre3/tables/matriceConf}
%\input{./Chapitre3/figures/precisionRappel}

Cette présentation permet notamment de visualiser si une classe est mieux prédite que d'autres. Il est facile de relever si le système est performant en se basant sur la diagonale, qui regroupe les vrais positifs et donc de calculer l'accuracy, tandis que les autres cases correspondent à des erreurs du système.

Cette matrice de confusion permet également de calculer le rappel et la précision de chaque classe. La précision P, défini par l'équation~\ref{eq:precision}, correspond au nombre de classes correctement prédites parmi tous les documents. Le rappel R, défini par l'équation~\ref{eq:rappel} correspond au nombre de classes correctement prédites parmi tous les documents pertinents.

\begin{equation}
  P_i = \frac{\text{nb prediction vraie}_i}{\text{nb prediction vraie}_i + \text{nb prediction fausse}_i}
  \label{eq:precision}
\end{equation}

\begin{equation}
  R_i = \frac{\text{nb prediction vraie}_i}{\text{nb prediction vraie}_i + \text{nb non predit}_i}
  \label{eq:rappel}
\end{equation}

Bien que pratique, la matrice de confusion ne permet pas de donner un score unique pour le système.


\vspace{1cm}
% \\
\textit{Précision ou rappel pondéré et non-pondéré}

Afin d'avoir un score global de la classification des émotions, on peut utiliser la précision pondérée (WA pour weighted accuracy) comme dans les travaux de Lee et Tashev~\cite{Lee2015} ou de Han et al.~\cite{Han2014}. La précision pondérée est calculée en prenant la moyenne, sur toutes les classes, du rapport entre les prédictions correctes dans cette classe et les prédictions fausses comme défini par l'équation~\ref{eq:WA}. La précision non pondérée est la fraction d'instances prédites correctement (c'est-à-dire le nombre total de prédictions correctes, divisé par le nombre total d'instances). Si on se réfère à la matrice de confusion, il suffit d'additionner toute la diagonale et de diviser par le nombre total de documents.

\begin{equation}
  WA = \frac{\sum^n_{i=1}{P_i}}{i}
  \label{eq:WA}
\end{equation}

Cette mesure permet de calculer la performance d'un système de reconnaissance, mais elle ne prend pas en compte la différence de performance entre les classes. Ici la classe colère dans notre exemple n'est pas bien reconnue. Pourtant si on calcule la WA ($\frac{0.9+0.851+0.571}{3}$) on retrouve $0,774$ soit $77,4\%$ de bonne prédiction. Or les prédictions de la classe colère ont une précision de $57,1\%$.

Nous pouvons également utilisé le rappel moyen non-pondéré, comme lors des challenges INTERSPEECH~\cite{Schuller2009,Schuller2011}. Il est calculé selon l'équation~\ref{eq:UAR} et permet de déterminer combien d'éléments pertinents ont été retrouvés.

\begin{equation}
  UAR = \frac{\text{nb prediction vraie}}{\text{nb prediction vraie}+ \text{nb non predit}}
  \label{eq:UAR}
\end{equation}

\vspace{1cm}
\textit{F-mesure}

La F-mesure, ou F-score en anglais, combine à la fois la précision et le rappel selon l'équation~\ref{eq:Fmesure}. Elle est comprise entre 0 et 1. Plus elle est grande, plus le système évalué est performant.

\begin{equation}
  F = 2 \left( \frac{precision.rappel}{precision+rappel} \right)
  \label{eq:Fmesure}
\end{equation}

Néanmoins toutes les reconnaissances d'émotion ne se font pas sur des émotions discrètes, il existe donc d'autres indicateurs utilisés pour mesurer la performance des systèmes.

\subsection{Tâche de régression : l'erreur quadratique moyenne}
L'erreur quadratique moyenne est une métrique qui est utilisée dans l'évaluation des systèmes de reconnaissance d'émotions continues~\cite{AVEC2017}. Elle permet de calculer un score d'accord entre deux séries temporelles, ici les annotations de références et les prédictions du système. Afin que cette métrique soit de la même dimension que les valeurs de référence, on utilise principalement la racine de l'erreur quadratique moyenne, RMSE pour \textit{Root Mean Square Error}. Cette métrique se calcule selon l'équation~\ref{eq:RMSE_score} entre les valeurs prédites $x_i$  et les valeurs de références $y_i$. $n$ correspond au nombre de valeurs.

\begin{equation}
    RMSE = \sqrt{\frac{1}{n}\Sigma_{i=1}^{n}{\Big(y_i - x_i\Big)^2}}
\label{eq:RMSE_score}
\end{equation}

Elle est comprise entre 0 et 1. Comme il s'agit d'un taux d'erreur, les systèmes à haute performance se rapproche de 0, ce qui indique un ajustement parfait entre les références et les prédictions. Cette métrique n'est pas exempte d'inconvénients. En effet, elle est très sensible aux valeurs extrêmes et elle est difficilement comparable à d'autre scores calculés sur des valeurs d'ordre de grandeur différent.

\subsubsection{Coefficient de Corrélation de Concordance}
Le coefficient de corrélation de concordance~\cite{CCC}, CCC pour Concordance Correlation Coefficient, a été établi comme un standard d'évaluation lors notamment des trois précédents Audio/Visual Emotion Challenge and Workshop (AVEC)~\cite{AVEC2017,AVEC2018,AVEC2019}. Cette métrique évalue l'accord entre deux séries temporelles selon l'équation~\ref{eq:CCC_score}, où $x$ et $y$ sont les deux variables, dans notre cas la prédiction et la référence. $\mu_x$, $\mu_y$ correspondent à leurs moyennes et $\sigma_x$, $\sigma_y$ à leur écart-type.

 \begin{equation}
    CCC = \frac{2\rho\sigma_x\sigma_y}{\sigma_x^2 + \sigma_y^2 + (\mu_x - \mu_y)^2}
 \label{eq:CCC_score}
 \end{equation}

Plus le CCC s'approche de 1 et plus le système est considéré comme performant. A l'inverse, plus le score s'approche de 0 et moins il y a de corrélation entre les prédictions et les références, dénotant un système peu performant.

Cette métrique sera utilisé dans les travaux de cette thèse pour évaluer la performance des systèmes de prédiction continue de la satisfaction. Afin de nuancer les différences de scores entre différentes configurations de nos systèmes, nous avons mis en place un intervalle de confiance, qui est explicité dans le chapitre~\ref{chapitre6}.

\section{Fusion de modalités}
Afin de pouvoir profiter à la fois des informations acoustiques et linguistiques, il est courant de fusionner ces deux modalités pour avoir un système performant et plus robuste~\cite{Wollmer2013,Alam2014,Atrey2010,Liu2018}. La fusion de modalités est assez vaste: il est possible également d'inclure des informations issues de vidéos ou de capteurs physiologiques, dont nous avons parlé au premier chapitre.

Dans le cadre de cette thèse, nous sommes principalement intéressés par les modalités acoustiques et linguistiques, puisque le reste des modalités ne peuvent pas être récupérées depuis les centres d'appels.
\\
\\
\textbf{Type de fusion}

La fusion peut s'effectuer de différentes manières.
\begin{itemize}
  \item Fusion des features~\cite{Wollmer2013,Alam2014,Atrey2010} : La fusion s'opère au niveau des features, on concatène les vecteurs représentant les différentes modalités. Cette méthode augmente le nombre de features en entrée du système et peut donner des résultats très différents en fonction de la stratégie de normalisation des données. En effet, il peut être compliqué pour le système de comprendre que les données représentent deux espaces différents. Donc il est courant de normaliser les données afin de se référer à un seul espace.
  \item Fusion des modèles~\cite{Atrey2010,Liu2018} : Plusieurs apprentissages sont fait de façon distincts pour chaque modalité jusqu'à une certaine couche dans le réseau de neurone. Les couches sont alors fusionnées, et l'apprentissage continue. Plus la fusion arrive tôt et plus le système doit en théorie avoir un bon pouvoir de généralisation.
  \item Fusion de décision~\cite{Wollmer2013,Atrey2010} : Plusieurs apprentissages sont fait de façon distincts pour chaque modalité. On prend la prédiction de chacun des modèles et on les fusionne. S'il s'agit d'une classification, on peut fusionner par vote majoritaire par exemple. S'il s'agit d'une régression, on peut faire la moyenne des sorties. De plus, on peut facilement mettre plus d'importance sur une des modalités en faisant une moyenne pondérée des sorties.
\end{itemize}

\section{Notre référence : AVEC}

\input{./Chapitre3/tables/avec}
Si nous nous replaçons dans le contexte de la thèse, nous cherchons à construire et donc à évaluer un système de reconnaissance des émotions continues depuis la parole. Dans ce cadre, nous avons recherché dans la littérature, des systèmes et des expérimentations qui soient comparables à nos recherches. Nous avons choisi de nous référer aux campagnes AVEC, \textit{Audio/Visual Emotion Challenge and Workshop}.

Ce challenge, qui en était à sa huitième itération en 2018, vise à comparer les méthodes de traitement multimédia et d'apprentissage automatique pour l'analyse automatique de la santé et des émotions dans les modalités audio et visuelles. Ces campagnes sont divisées en différents objectifs qui gravitent autour des émotions: de la détection de dépression, de bipolarité, d'état d'esprit ou d'émotions issues de différentes cultures par exemple. Comme ces campagnes s'appuient sur des corpus multimodaux comme RECOLA~\cite{Ringeval2013} et SEWA~\cite{SEWA} notamment, les émotions peuvent être détectées à partir de différentes modalités, notamment depuis la parole et les expressions faciales.

Afin de pouvoir nous comparer à l'état de l'art, nous avons décidé de comparer nos résultats à ceux obtenus sur le corpus SEWA, dont nous avons parlé dans ce chapitre.
Ce corpus considère trois dimensions émotionnelles : la valence, l'activation et le \textit{liking}. Il est utilisé notamment dans les campagnes AVEC depuis 2017~\cite{AVEC2017,AVEC2018,AVEC2019} et sert actuellement de baseline dans la communauté. Nous nous intéressons donc à la tâche de régression audio uniquement.

Nous résumons dans le tableau~\ref{tab:avec}, différents résultats obtenus qui utilisent la partie Allemande et Hongroise du corpus, soit celle à laquelle nous avons accès. Comme nous pouvons le voir, de nombreux ensembles de features et types d'algorithme ont été utilisés pour la reconnaissance des émotions depuis la parole pour le corpus SEWA.
Pour ce qui est de la modalité acoustique, nous pouvons voir que les systèmes les plus performants ont des scores de 0.571 pour l'activation, 0.561 pour la valence et 0.335 pour le liking. Nous voyons que la plupart des participants ont utilisés des systèmes CNN ou LSTM pour résoudre la régression. En regardant les deux travaux de Huang et al.~\cite{Huang2017,Huang2018}, nous pouvons remarquer que le choix de l'ensemble de descripteurs influence fortement sur les scores des systèmes de regression. On remarque également une prédominance de l'utilisation des eGeMAPS, qui donne les meilleurs scores.

Pour ce qui est de la modalité linguistique, on peut notamment remarquer que l'on retrouve principalement l'utilisation de plongements de mots (word2vec et GloVe) et le même type d'architecture que pour la modalité acoustique, avec une variation du réseau LSTM, qui possède deux couches pour les expérimentations sur le liking.

Si on compare les résultats à ceux trouvés par la modalité acoustique, les scores maximum sont assez similaires : 0.597 contre 0.571 pour l'activation, 0.600 contre 0.561 pour la valence. On observe également une nette amélioration pour le liking : 0.480 au lieu de 0.335. En général, on trouve des scores un peu plus élevés en utilisant la modalité linguistique~\cite{Gunes2013}. Cela peut être du au fait que les descripteurs utilisés sont plus adaptés ou que les architectures neuronales sont plus adaptées à ce type de données.

Dans le cadre de cette thèse, nous cherchons à mettre en place des solutions neuronales profondes. Nous avons donc fait le choix de nous comparer aux résultats obtenus par Schmitt et al.~\cite{Schmitt2019}.
%\input{./Chapitre3/tables/avectexte}

% \section{Sentiment Analysis et Opinion Mining}
% Utiliser le texte pour déterminer l'état émotionnel est une voie de plus en plus empruntée grâce, notamment, à la performance des systèmes de reconnaissance de la parole actuels.
% Des domaines tels que l'Analyse de Sentiments et la Détection d'Opinion, utilise des données textuelles de type livres, témoignages ou plus récemment des tweets et  des contenus en ligne afin d'extraire des informations. On peut également considérer la transcription automatique comme un élément associé à la parole, et donc y retrouver des marqueurs de l'émotion.
%
% Dans ces domaines, on cherche alors l'émotion dans l'aspect sémantique de la phrase, dans les diffluences ou les répétitions.
%
% Le terme \textit{Détection d'Opinion} (traduction de Opinion Mining) a été popularisé par les travaux de Kushal Dave~\cite{Dave2003}, définissant le domaine comme traitant un ensemble de données pour en retirer ses qualités et ses caractéristiques afin de statuer sur l'avis du sujet~\cite{Pang2008}. Le domaine \textit{Analyse de Sentiments} (traduction de Affect Analysis) a pris de l'ampleur depuis les travaux de Sanjiv Das, Mike Chen et Richard Tong au début du XXIe siècle~\cite{Das2007,Tong2001}. Il diffère de l'Opinion Mining : la majorité de ces travaux se concentrent sur la classification des avis ou de phrases en polarité (positif ou négatif). Mais ces domaines de recherche restent très fortement liés et constituent la plus grande partie de recherche d'indices émotionnelles à base de données textuelles.
%
% Des outils tel que des dictionnaires de polarité, FAN~\cite{Monnier2014} par exemple, permettent de colorer émotionnellement des mots ou des phrases, ou encore des analyseurs de syntaxe tel que Macaon~\cite{Nasr2011} permettent d'étudier la structure d'un énoncé pour faire ressortir sa forme morphosyntaxique. Ces outils sont utilisés dans la reconnaissance d'émotions que ce soit à l'échelle d'un segment de parole, d'une phrase ou d'un document.
%
% \subsection{Corpus existant}
% De nombreux corpus de texte existent, mais peu sont annotés en émotion. Si nous nous référons aux campagnes AVEC, la transcription automatique de la parole est utilisée en tant que donnée textuelle. Cette transcription est ensuite alignée aux annotations, permettant d'obtenir ainsi des données annotées.
%
% Il est également possible de constituer son propre corpus en confrontant un texte à des lexiques d'émotions comme par exemple le lexique NRC~\cite{Mohammad2013} disponible dans 105 langues, SentiWordNet~\cite{Sebastiani2006} ou encore le lexique FAN~\cite{Monnier2014}.
% Le lexique NRC contient 14 182 mots et sont associés à 10 catégories différentes (anticipation, colère, tristesse, joie, peur, confiance, dégoût, surprise, positive, négative). Chaque mot peut être associé à plusieurs catégories ou à aucune.
% Le dictionnaire French Affective Norms (FAN) propose quant à lui, un score de valence (entre 0 et 10) pour plus de 1000 mots, annotés par plus de 400 participants.
%
% Il est également possible de se tourner vers des corpus déjà construits, comme ceux utilisés lors des campagnes SemEval. Par exemple des titres d'article provenant de journaux~\cite{Strapparava2010} ou encore un corpus constitué de livres pour enfants~\cite{Etienne2020}. Les données de SemEval sont annotés en catégories discrètes (colère, dégoût, peur, joie, tristesse, surprise et valence) par une échelle allant de 0 à 100 pour chaque émotion. 0 signifiant l'absence de l'émotion dans le titre et 100 la charge émotionnelle maximale. La valence varie entre -100 (négative) et 100 (positive) avec 0 (neutre) au milieu. Le corpus de livres pour enfants est annoté en 10 catégories discrètes (colère, dégoût, joie, peur, surprise, tristesse,culpabilité, embarras, fierté et jalousie). Cette annotation est enrichie en plusieurs concepts, notamment le mode d'expression de l'émotion : désignée, comportementale, montrée, étayée.
%
% De nombreux autres corpus existent afin de traiter un maximum de tâches en lien avec les émotions. Mais ces corpus sont difficilement utilisables en l'état, il est important de les transformer afin de les rendre compréhensibles par les systèmes d'apprentissage.

% \subsection{Les scores de systèmes à l'état de l'art tirés d'AVEC}
%
% \input{./Chapitre3/tables/avecmulti}
%
% Une fois encore, nous nous intéressons aux scores présentés lors du challenge AVEC. La multi-modalité étant un aspect prédominant dans cette campagne, les données comportent également des vidéos qui peuvent être utilisés par les participants. Dans le tableau~\ref{tab:avecmulti}, nous ne rapportons que les scores des modalités acoustiques et linguistiques.

\section{Conclusion}
Dans ce chapitre, nous avons résumé les principaux composants de la reconnaissance des émotions depuis la parole, sans oublier la reconnaissance depuis le texte. A partir de ces connaissances, nous avons pu établir un référentiel sur la tâche que nous cherchons à accomplir. En effet, nous avons fait le choix de nous comparer au corpus SEWA et aux différents systèmes et features utilisés avec celui-ci. De plus, nous avons introduit le principe de fusion des modalités, qui apparaîtra dans les contributions de cette thèse.

Dans la prochaine partie, nous allons nous concentrer sur les contributions de cette thèse : de la construction d'un corpus répondant à nos besoins, à la mise en place de systèmes de reconnaissance des émotions performants.
